# -*- coding: utf-8 -*-
"""drl_trading_system.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10PKAxUNUCvnor7KK7RS8LV7-Kh9YAjPi
"""

# Deep Reinforcement Learning Trading System
# Complete Implementation with PPO, A2C, and DQN agents

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import TimeSeriesSplit
import warnings
warnings.filterwarnings('ignore')

# Deep Learning libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from collections import deque
import random
import json
import os
from datetime import datetime

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Configure plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("Deep Reinforcement Learning Trading System")
print("=" * 60)

# ============================================================================
# 1. DATA COLLECTION AND PREPROCESSING (From Assignment 1)
# ============================================================================

class DataPipeline:
    def __init__(self, assets=['AAPL', 'MSFT', 'GOOGL', 'TSLA'], period='6y'):
        self.assets = assets
        self.period = period
        self.raw_data = {}
        self.processed_data = {}
        self.scalers = {}

    def fetch_data(self):
        """Fetch historical data for all assets"""
        print("Fetching historical data...")
        for symbol in self.assets:
            try:
                ticker = yf.Ticker(symbol)
                df = ticker.history(period=self.period)
                if not df.empty and len(df) >= 1500:
                    self.raw_data[symbol] = df
                    print(f"✓ {symbol}: {len(df)} records")
                else:
                    print(f"✗ Insufficient data for {symbol}")
            except Exception as e:
                print(f"✗ Error fetching {symbol}: {str(e)}")

    def calculate_technical_indicators(self, df):
        """Calculate comprehensive technical indicators"""
        data = df.copy()

        # Moving Averages
        data['SMA_20'] = data['Close'].rolling(window=20).mean()
        data['EMA_20'] = data['Close'].ewm(span=20).mean()
        data['SMA_50'] = data['Close'].rolling(window=50).mean()

        # RSI
        def calculate_rsi(prices, window=14):
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
            rs = gain / loss
            return 100 - (100 / (1 + rs))

        data['RSI'] = calculate_rsi(data['Close'])

        # MACD
        ema_12 = data['Close'].ewm(span=12).mean()
        ema_26 = data['Close'].ewm(span=26).mean()
        data['MACD'] = ema_12 - ema_26
        data['MACD_signal'] = data['MACD'].ewm(span=9).mean()
        data['MACD_hist'] = data['MACD'] - data['MACD_signal']

        # Bollinger Bands
        data['BB_middle'] = data['Close'].rolling(window=20).mean()
        bb_std = data['Close'].rolling(window=20).std()
        data['BB_upper'] = data['BB_middle'] + (bb_std * 2)
        data['BB_lower'] = data['BB_middle'] - (bb_std * 2)
        data['BB_width'] = data['BB_upper'] - data['BB_lower']
        data['BB_position'] = (data['Close'] - data['BB_lower']) / (data['BB_upper'] - data['BB_lower'])

        # ATR
        high_low = data['High'] - data['Low']
        high_close = np.abs(data['High'] - data['Close'].shift())
        low_close = np.abs(data['Low'] - data['Close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        data['ATR'] = true_range.rolling(14).mean()

        # Stochastic Oscillator
        low_min = data['Low'].rolling(window=14).min()
        high_max = data['High'].rolling(window=14).max()
        data['STOCH_K'] = 100 * (data['Close'] - low_min) / (high_max - low_min)
        data['STOCH_D'] = data['STOCH_K'].rolling(window=3).mean()

        # Williams %R
        data['Williams_R'] = -100 * (high_max - data['Close']) / (high_max - low_min)

        # Volume indicators
        data['Volume_SMA'] = data['Volume'].rolling(window=20).mean()
        data['Volume_ratio'] = data['Volume'] / data['Volume_SMA']

        # Price-based features
        data['Price_change'] = data['Close'].pct_change()
        data['High_Low_ratio'] = data['High'] / data['Low']
        data['Close_Open_ratio'] = data['Close'] / data['Open']

        # Additional DRL-specific features
        data['momentum_5'] = data['Close'].pct_change(5)
        data['momentum_10'] = data['Close'].pct_change(10)
        data['vol_regime'] = data['ATR'].rolling(20).mean() / data['ATR'].rolling(60).mean()
        data['trend_strength'] = abs(data['MACD'] - data['MACD_signal'])
        data['mean_reversion'] = (data['Close'] - data['SMA_20']) / data['ATR']
        data['risk_adj_return'] = data['Price_change'] / data['ATR']
        data['relative_volume'] = data['Volume'] / data['Volume'].rolling(20).mean()

        return data

    def preprocess_data(self, df):
        """Preprocess and normalize data"""
        df_clean = df.dropna()

        # Separate price and technical indicators
        price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        tech_cols = [col for col in df_clean.columns if col not in price_cols]

        # Scale features
        price_scaler = MinMaxScaler()
        tech_scaler = StandardScaler()

        df_normalized = df_clean.copy()
        df_normalized[price_cols] = price_scaler.fit_transform(df_clean[price_cols])

        if len(tech_cols) > 0:
            df_normalized[tech_cols] = tech_scaler.fit_transform(df_clean[tech_cols])

        return df_normalized, df_clean, price_scaler, tech_scaler

    def process_all_assets(self):
        """Process all assets with feature engineering"""
        print("Processing assets with feature engineering...")

        for symbol, df in self.raw_data.items():
            print(f"Processing {symbol}...")

            # Calculate technical indicators
            engineered_df = self.calculate_technical_indicators(df)

            # Preprocess data
            normalized, clean, p_scaler, t_scaler = self.preprocess_data(engineered_df)

            self.processed_data[symbol] = {
                'normalized': normalized,
                'clean': clean,
                'raw': df
            }

            self.scalers[symbol] = {
                'price_scaler': p_scaler,
                'tech_scaler': t_scaler
            }

            print(f"✓ {symbol}: {len(clean)} samples, {len(clean.columns)} features")

# ============================================================================
# 2. TRADING ENVIRONMENT
# ============================================================================

class TradingEnvironment:
    def __init__(self, data, initial_balance=10000, transaction_cost=0.001):
        self.data = data.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.current_step = 0
        self.balance = initial_balance
        self.shares_held = 0
        self.total_shares_sold = 0
        self.total_sales_value = 0
        self.net_worth = initial_balance
        self.max_net_worth = initial_balance
        self.trades = []

        # State features (excluding OHLCV for state representation)
        self.feature_columns = [col for col in data.columns
                               if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]

        # Action space: 0=Hold, 1=Buy, 2=Sell
        self.action_space = 3
        self.observation_space = len(self.feature_columns) + 3  # +3 for portfolio state

    def reset(self):
        """Reset environment to initial state"""
        self.current_step = 0
        self.balance = self.initial_balance
        self.shares_held = 0
        self.total_shares_sold = 0
        self.total_sales_value = 0
        self.net_worth = self.initial_balance
        self.max_net_worth = self.initial_balance
        self.trades = []

        return self._get_observation()

    def _get_observation(self):
        """Get current observation state"""
        if self.current_step >= len(self.data):
            return np.zeros(self.observation_space)

        # Technical indicators
        tech_features = self.data[self.feature_columns].iloc[self.current_step].values

        # Portfolio state
        current_price = self.data['Close'].iloc[self.current_step]
        portfolio_value = self.balance + self.shares_held * current_price

        portfolio_state = np.array([
            self.balance / self.initial_balance,  # Normalized balance
            self.shares_held * current_price / self.initial_balance,  # Normalized holdings value
            portfolio_value / self.initial_balance  # Normalized total value
        ])

        # Handle NaN values
        tech_features = np.nan_to_num(tech_features, nan=0.0)
        portfolio_state = np.nan_to_num(portfolio_state, nan=0.0)

        return np.concatenate([tech_features, portfolio_state])

    def step(self, action):
        """Execute action and return next state, reward, done"""
        if self.current_step >= len(self.data) - 1:
            return self._get_observation(), 0, True, {}

        current_price = self.data['Close'].iloc[self.current_step]

        # Execute action
        if action == 1:  # Buy
            shares_to_buy = self.balance // (current_price * (1 + self.transaction_cost))
            if shares_to_buy > 0:
                cost = shares_to_buy * current_price * (1 + self.transaction_cost)
                self.balance -= cost
                self.shares_held += shares_to_buy
                self.trades.append({
                    'step': self.current_step,
                    'action': 'BUY',
                    'shares': shares_to_buy,
                    'price': current_price,
                    'cost': cost
                })

        elif action == 2:  # Sell
            if self.shares_held > 0:
                revenue = self.shares_held * current_price * (1 - self.transaction_cost)
                self.balance += revenue
                self.total_sales_value += revenue
                self.total_shares_sold += self.shares_held
                self.trades.append({
                    'step': self.current_step,
                    'action': 'SELL',
                    'shares': self.shares_held,
                    'price': current_price,
                    'revenue': revenue
                })
                self.shares_held = 0

        # Move to next step
        self.current_step += 1

        # Calculate reward
        new_net_worth = self.balance + self.shares_held * self.data['Close'].iloc[self.current_step]
        reward = (new_net_worth - self.net_worth) / self.initial_balance

        # Update tracking variables
        self.net_worth = new_net_worth
        self.max_net_worth = max(self.max_net_worth, new_net_worth)

        # Check if done
        done = self.current_step >= len(self.data) - 1

        return self._get_observation(), reward, done, {
            'net_worth': self.net_worth,
            'balance': self.balance,
            'shares_held': self.shares_held
        }

    def get_performance_metrics(self):
        """Calculate performance metrics"""
        if len(self.trades) == 0:
            return {
                'total_return': 0,
                'num_trades': 0,
                'win_rate': 0,
                'sharpe_ratio': 0,
                'max_drawdown': 0
            }

        total_return = (self.net_worth - self.initial_balance) / self.initial_balance

        # Calculate other metrics
        returns = []
        for i in range(1, len(self.data)):
            if i <= self.current_step:
                price_change = (self.data['Close'].iloc[i] - self.data['Close'].iloc[i-1]) / self.data['Close'].iloc[i-1]
                returns.append(price_change)

        returns = np.array(returns)
        sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0

        # Max drawdown
        max_drawdown = (self.max_net_worth - self.net_worth) / self.max_net_worth

        return {
            'total_return': total_return,
            'num_trades': len(self.trades),
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'final_balance': self.balance,
            'final_shares': self.shares_held,
            'net_worth': self.net_worth
        }

# ============================================================================
# 3. DEEP REINFORCEMENT LEARNING AGENTS
# ============================================================================

class DQNAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = learning_rate

        # Neural Network
        self.q_network = self._build_network()
        self.target_network = self._build_network()
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # Update target network
        self.update_target_network()

    def _build_network(self):
        """Build DQN neural network"""
        model = nn.Sequential(
            nn.Linear(self.state_size, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, self.action_size)
        )
        return model

    def update_target_network(self):
        """Update target network with main network weights"""
        self.target_network.load_state_dict(self.q_network.state_dict())

    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        """Choose action using epsilon-greedy policy"""
        if np.random.random() <= self.epsilon:
            return random.randrange(self.action_size)

        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.q_network(state_tensor)
        return np.argmax(q_values.cpu().data.numpy())

    def replay(self, batch_size=32):
        """Train the model on a batch of experiences"""
        if len(self.memory) < batch_size:
            return

        batch = random.sample(self.memory, batch_size)
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.BoolTensor([e[4] for e in batch])

        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (0.95 * next_q_values * ~dones)

        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

class PPOAgent:
    def __init__(self, state_size, action_size, learning_rate=0.0003):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate

        # Actor-Critic networks
        self.actor = self._build_actor()
        self.critic = self._build_critic()
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)

        # PPO hyperparameters
        self.clip_epsilon = 0.2
        self.ppo_epochs = 4
        self.entropy_coef = 0.01

    def _build_actor(self):
        """Build actor network"""
        return nn.Sequential(
            nn.Linear(self.state_size, 128),
            nn.Tanh(),
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, self.action_size),
            nn.Softmax(dim=-1)
        )

    def _build_critic(self):
        """Build critic network"""
        return nn.Sequential(
            nn.Linear(self.state_size, 128),
            nn.Tanh(),
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )

    def act(self, state):
        """Choose action using policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.actor(state_tensor)
        dist = Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action).item()

    def evaluate(self, state, action):
        """Evaluate state-action pair"""
        state_tensor = torch.FloatTensor(state)
        action_tensor = torch.LongTensor(action)

        action_probs = self.actor(state_tensor)
        dist = Categorical(action_probs)
        action_log_probs = dist.log_prob(action_tensor)
        entropy = dist.entropy()

        state_value = self.critic(state_tensor)

        return action_log_probs, state_value, entropy

    def update(self, states, actions, rewards, log_probs, values, next_values):
        """Update policy using PPO"""
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        old_log_probs = torch.FloatTensor(log_probs)
        values = torch.FloatTensor(values)
        next_values = torch.FloatTensor(next_values)

        # Calculate advantages
        advantages = rewards + 0.99 * next_values - values
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # PPO update
        for _ in range(self.ppo_epochs):
            log_probs_new, values_new, entropy = self.evaluate(states, actions)

            # Actor loss
            ratio = torch.exp(log_probs_new - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy.mean()

            # Critic loss
            critic_loss = F.mse_loss(values_new.squeeze(), rewards + 0.99 * next_values)

            # Update networks
            self.actor_optimizer.zero_grad()
            actor_loss.backward(retain_graph=True)
            self.actor_optimizer.step()

            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()

# ============================================================================
# 4. TRAINING FRAMEWORK
# ============================================================================

class TradingTrainer:
    def __init__(self, data_pipeline, agent_type='DQN'):
        self.data_pipeline = data_pipeline
        self.agent_type = agent_type
        self.results = {}

    def train_agent(self, symbol, episodes=1000):
        """Train agent on specific symbol"""
        print(f"\nTraining {self.agent_type} agent on {symbol}...")

        # Prepare data
        data = self.data_pipeline.processed_data[symbol]['normalized']

        # Split data
        train_size = int(len(data) * 0.8)
        train_data = data[:train_size]
        test_data = data[train_size:]

        # Create environment
        env = TradingEnvironment(train_data)

        # Initialize agent
        if self.agent_type == 'DQN':
            agent = DQNAgent(env.observation_space, env.action_space)
        elif self.agent_type == 'PPO':
            agent = PPOAgent(env.observation_space, env.action_space)
        else:
            raise ValueError(f"Unknown agent type: {self.agent_type}")

        # Training loop
        episode_rewards = []
        episode_returns = []

        for episode in range(episodes):
            state = env.reset()
            episode_reward = 0
            done = False

            # For PPO
            if self.agent_type == 'PPO':
                states, actions, rewards, log_probs, values = [], [], [], [], []

            while not done:
                if self.agent_type == 'DQN':
                    action = agent.act(state)
                    next_state, reward, done, _ = env.step(action)
                    agent.remember(state, action, reward, next_state, done)
                    state = next_state
                    episode_reward += reward

                    if len(agent.memory) > 32:
                        agent.replay()

                elif self.agent_type == 'PPO':
                    action, log_prob = agent.act(state)
                    next_state, reward, done, _ = env.step(action)

                    # Store transition
                    states.append(state)
                    actions.append(action)
                    rewards.append(reward)
                    log_probs.append(log_prob)
                    values.append(agent.critic(torch.FloatTensor(state).unsqueeze(0)).item())

                    state = next_state
                    episode_reward += reward

            # Update PPO agent
            if self.agent_type == 'PPO' and len(states) > 0:
                next_values = values[1:] + [0]  # Bootstrap with 0 for terminal state
                agent.update(states, actions, rewards, log_probs, values, next_values)

            # Update target network for DQN
            if self.agent_type == 'DQN' and episode % 100 == 0:
                agent.update_target_network()

            # Track progress
            episode_rewards.append(episode_reward)
            performance = env.get_performance_metrics()
            episode_returns.append(performance['total_return'])

            if episode % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_return = np.mean(episode_returns[-100:])
                print(f"Episode {episode}, Avg Reward: {avg_reward:.4f}, Avg Return: {avg_return:.4f}")

        # Test on unseen data
        print(f"Testing {self.agent_type} agent on {symbol}...")
        test_env = TradingEnvironment(test_data)
        test_state = test_env.reset()
        test_done = False

        while not test_done:
            if self.agent_type == 'DQN':
                # Use trained policy without exploration
                agent.epsilon = 0
                test_action = agent.act(test_state)
            else:  # PPO
                test_action, _ = agent.act(test_state)

            test_state, _, test_done, _ = test_env.step(test_action)

        # Store results
        test_performance = test_env.get_performance_metrics()
        self.results[symbol] = {
            'agent_type': self.agent_type,
            'training_rewards': episode_rewards,
            'training_returns': episode_returns,
            'test_performance': test_performance,
            'test_env': test_env,
            'agent': agent
        }

        print(f"✓ {symbol} - Test Return: {test_performance['total_return']:.4f}")
        return agent, test_performance

# ============================================================================
# 5. MAIN EXECUTION
# ============================================================================

def main():
    # Initialize data pipeline
    pipeline = DataPipeline()

    # Fetch and process data
    pipeline.fetch_data()
    pipeline.process_all_assets()

    # Train agents for each asset
    results = {}

    for agent_type in ['DQN', 'PPO']:
        print(f"\n{'='*60}")
        print(f"Training {agent_type} agents")
        print('='*60)

        trainer = TradingTrainer(pipeline, agent_type)

        for symbol in pipeline.assets:
            if symbol in pipeline.processed_data:
                agent, performance = trainer.train_agent(symbol, episodes=500)
                results[f"{agent_type}_{symbol}"] = {
                    'agent': agent,
                    'performance': performance,
                    'trainer': trainer
                }

    # Display results summary
    print(f"\n{'='*60}")
    print("FINAL RESULTS SUMMARY")
    print('='*60)

    for key, result in results.items():
        agent_type, symbol = key.split('_', 1)
        performance = result['performance']
        print(f"{agent_type} - {symbol}:")
        print(f"  Total Return: {performance['total_return']:.4f}")
        print(f"  Sharpe Ratio: {performance['sharpe_ratio']:.4f}")
        print(f"  Number of Trades: {performance['num_trades']}")
        print(f"  Max Drawdown: {performance['max_drawdown']:.4f}")
        print()

    return pipeline, results

if __name__ == "__main__":
    pipeline, results = main()
    print("Training completed! Results stored in 'results' variable.")

