{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGSlsMouKNFR",
        "outputId": "353fcdd7-26a7-4ce6-f379-e8b037a7ac22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Reinforcement Learning Trading System\n",
            "============================================================\n",
            "Fetching historical data...\n",
            "✓ AAPL: 1508 records\n",
            "✓ MSFT: 1508 records\n",
            "✓ GOOGL: 1508 records\n",
            "✓ TSLA: 1508 records\n",
            "Processing assets with feature engineering...\n",
            "Processing AAPL...\n",
            "✓ AAPL: 1436 samples, 35 features\n",
            "Processing MSFT...\n",
            "✓ MSFT: 1436 samples, 35 features\n",
            "Processing GOOGL...\n",
            "✓ GOOGL: 1436 samples, 35 features\n",
            "Processing TSLA...\n",
            "✓ TSLA: 1436 samples, 35 features\n",
            "\n",
            "============================================================\n",
            "Training DQN agents\n",
            "============================================================\n",
            "\n",
            "Training DQN agent on AAPL...\n",
            "Episode 0, Avg Reward: 3.4951, Avg Return: 3.4951\n",
            "Episode 100, Avg Reward: nan, Avg Return: nan\n",
            "Episode 200, Avg Reward: 2.4484, Avg Return: 2.4484\n",
            "Episode 300, Avg Reward: nan, Avg Return: nan\n",
            "Episode 400, Avg Reward: 2.3051, Avg Return: 2.3051\n",
            "Testing DQN agent on AAPL...\n",
            "✓ AAPL - Test Return: 0.0000\n",
            "\n",
            "Training DQN agent on MSFT...\n",
            "Episode 0, Avg Reward: -0.8279, Avg Return: -0.8279\n",
            "Episode 100, Avg Reward: nan, Avg Return: nan\n"
          ]
        }
      ],
      "source": [
        "# Deep Reinforcement Learning Trading System\n",
        "# Complete Implementation with PPO, A2C, and DQN agents\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from collections import deque\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Deep Reinforcement Learning Trading System\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATA COLLECTION AND PREPROCESSING (From Assignment 1)\n",
        "# ============================================================================\n",
        "\n",
        "class DataPipeline:\n",
        "    def __init__(self, assets=['AAPL', 'MSFT', 'GOOGL', 'TSLA'], period='6y'):\n",
        "        self.assets = assets\n",
        "        self.period = period\n",
        "        self.raw_data = {}\n",
        "        self.processed_data = {}\n",
        "        self.scalers = {}\n",
        "\n",
        "    def fetch_data(self):\n",
        "        \"\"\"Fetch historical data for all assets\"\"\"\n",
        "        print(\"Fetching historical data...\")\n",
        "        for symbol in self.assets:\n",
        "            try:\n",
        "                ticker = yf.Ticker(symbol)\n",
        "                df = ticker.history(period=self.period)\n",
        "                if not df.empty and len(df) >= 1500:\n",
        "                    self.raw_data[symbol] = df\n",
        "                    print(f\"✓ {symbol}: {len(df)} records\")\n",
        "                else:\n",
        "                    print(f\"✗ Insufficient data for {symbol}\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error fetching {symbol}: {str(e)}\")\n",
        "\n",
        "    def calculate_technical_indicators(self, df):\n",
        "        \"\"\"Calculate comprehensive technical indicators\"\"\"\n",
        "        data = df.copy()\n",
        "\n",
        "        # Moving Averages\n",
        "        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
        "        data['EMA_20'] = data['Close'].ewm(span=20).mean()\n",
        "        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n",
        "\n",
        "        # RSI\n",
        "        def calculate_rsi(prices, window=14):\n",
        "            delta = prices.diff()\n",
        "            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "            rs = gain / loss\n",
        "            return 100 - (100 / (1 + rs))\n",
        "\n",
        "        data['RSI'] = calculate_rsi(data['Close'])\n",
        "\n",
        "        # MACD\n",
        "        ema_12 = data['Close'].ewm(span=12).mean()\n",
        "        ema_26 = data['Close'].ewm(span=26).mean()\n",
        "        data['MACD'] = ema_12 - ema_26\n",
        "        data['MACD_signal'] = data['MACD'].ewm(span=9).mean()\n",
        "        data['MACD_hist'] = data['MACD'] - data['MACD_signal']\n",
        "\n",
        "        # Bollinger Bands\n",
        "        data['BB_middle'] = data['Close'].rolling(window=20).mean()\n",
        "        bb_std = data['Close'].rolling(window=20).std()\n",
        "        data['BB_upper'] = data['BB_middle'] + (bb_std * 2)\n",
        "        data['BB_lower'] = data['BB_middle'] - (bb_std * 2)\n",
        "        data['BB_width'] = data['BB_upper'] - data['BB_lower']\n",
        "        data['BB_position'] = (data['Close'] - data['BB_lower']) / (data['BB_upper'] - data['BB_lower'])\n",
        "\n",
        "        # ATR\n",
        "        high_low = data['High'] - data['Low']\n",
        "        high_close = np.abs(data['High'] - data['Close'].shift())\n",
        "        low_close = np.abs(data['Low'] - data['Close'].shift())\n",
        "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
        "        true_range = np.max(ranges, axis=1)\n",
        "        data['ATR'] = true_range.rolling(14).mean()\n",
        "\n",
        "        # Stochastic Oscillator\n",
        "        low_min = data['Low'].rolling(window=14).min()\n",
        "        high_max = data['High'].rolling(window=14).max()\n",
        "        data['STOCH_K'] = 100 * (data['Close'] - low_min) / (high_max - low_min)\n",
        "        data['STOCH_D'] = data['STOCH_K'].rolling(window=3).mean()\n",
        "\n",
        "        # Williams %R\n",
        "        data['Williams_R'] = -100 * (high_max - data['Close']) / (high_max - low_min)\n",
        "\n",
        "        # Volume indicators\n",
        "        data['Volume_SMA'] = data['Volume'].rolling(window=20).mean()\n",
        "        data['Volume_ratio'] = data['Volume'] / data['Volume_SMA']\n",
        "\n",
        "        # Price-based features\n",
        "        data['Price_change'] = data['Close'].pct_change()\n",
        "        data['High_Low_ratio'] = data['High'] / data['Low']\n",
        "        data['Close_Open_ratio'] = data['Close'] / data['Open']\n",
        "\n",
        "        # Additional DRL-specific features\n",
        "        data['momentum_5'] = data['Close'].pct_change(5)\n",
        "        data['momentum_10'] = data['Close'].pct_change(10)\n",
        "        data['vol_regime'] = data['ATR'].rolling(20).mean() / data['ATR'].rolling(60).mean()\n",
        "        data['trend_strength'] = abs(data['MACD'] - data['MACD_signal'])\n",
        "        data['mean_reversion'] = (data['Close'] - data['SMA_20']) / data['ATR']\n",
        "        data['risk_adj_return'] = data['Price_change'] / data['ATR']\n",
        "        data['relative_volume'] = data['Volume'] / data['Volume'].rolling(20).mean()\n",
        "\n",
        "        return data\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        \"\"\"Preprocess and normalize data\"\"\"\n",
        "        df_clean = df.dropna()\n",
        "\n",
        "        # Separate price and technical indicators\n",
        "        price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "        tech_cols = [col for col in df_clean.columns if col not in price_cols]\n",
        "\n",
        "        # Scale features\n",
        "        price_scaler = MinMaxScaler()\n",
        "        tech_scaler = StandardScaler()\n",
        "\n",
        "        df_normalized = df_clean.copy()\n",
        "        df_normalized[price_cols] = price_scaler.fit_transform(df_clean[price_cols])\n",
        "\n",
        "        if len(tech_cols) > 0:\n",
        "            df_normalized[tech_cols] = tech_scaler.fit_transform(df_clean[tech_cols])\n",
        "\n",
        "        return df_normalized, df_clean, price_scaler, tech_scaler\n",
        "\n",
        "    def process_all_assets(self):\n",
        "        \"\"\"Process all assets with feature engineering\"\"\"\n",
        "        print(\"Processing assets with feature engineering...\")\n",
        "\n",
        "        for symbol, df in self.raw_data.items():\n",
        "            print(f\"Processing {symbol}...\")\n",
        "\n",
        "            # Calculate technical indicators\n",
        "            engineered_df = self.calculate_technical_indicators(df)\n",
        "\n",
        "            # Preprocess data\n",
        "            normalized, clean, p_scaler, t_scaler = self.preprocess_data(engineered_df)\n",
        "\n",
        "            self.processed_data[symbol] = {\n",
        "                'normalized': normalized,\n",
        "                'clean': clean,\n",
        "                'raw': df\n",
        "            }\n",
        "\n",
        "            self.scalers[symbol] = {\n",
        "                'price_scaler': p_scaler,\n",
        "                'tech_scaler': t_scaler\n",
        "            }\n",
        "\n",
        "            print(f\"✓ {symbol}: {len(clean)} samples, {len(clean.columns)} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. TRADING ENVIRONMENT\n",
        "# ============================================================================\n",
        "\n",
        "class TradingEnvironment:\n",
        "    def __init__(self, data, initial_balance=10000, transaction_cost=0.001):\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.initial_balance = initial_balance\n",
        "        self.transaction_cost = transaction_cost\n",
        "        self.current_step = 0\n",
        "        self.balance = initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_sold = 0\n",
        "        self.total_sales_value = 0\n",
        "        self.net_worth = initial_balance\n",
        "        self.max_net_worth = initial_balance\n",
        "        self.trades = []\n",
        "\n",
        "        # State features (excluding OHLCV for state representation)\n",
        "        self.feature_columns = [col for col in data.columns\n",
        "                               if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "        # Action space: 0=Hold, 1=Buy, 2=Sell\n",
        "        self.action_space = 3\n",
        "        self.observation_space = len(self.feature_columns) + 3  # +3 for portfolio state\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        self.current_step = 0\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_sold = 0\n",
        "        self.total_sales_value = 0\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.max_net_worth = self.initial_balance\n",
        "        self.trades = []\n",
        "\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        \"\"\"Get current observation state\"\"\"\n",
        "        if self.current_step >= len(self.data):\n",
        "            return np.zeros(self.observation_space)\n",
        "\n",
        "        # Technical indicators\n",
        "        tech_features = self.data[self.feature_columns].iloc[self.current_step].values\n",
        "\n",
        "        # Portfolio state\n",
        "        current_price = self.data['Close'].iloc[self.current_step]\n",
        "        portfolio_value = self.balance + self.shares_held * current_price\n",
        "\n",
        "        portfolio_state = np.array([\n",
        "            self.balance / self.initial_balance,  # Normalized balance\n",
        "            self.shares_held * current_price / self.initial_balance,  # Normalized holdings value\n",
        "            portfolio_value / self.initial_balance  # Normalized total value\n",
        "        ])\n",
        "\n",
        "        # Handle NaN values\n",
        "        tech_features = np.nan_to_num(tech_features, nan=0.0)\n",
        "        portfolio_state = np.nan_to_num(portfolio_state, nan=0.0)\n",
        "\n",
        "        return np.concatenate([tech_features, portfolio_state])\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state, reward, done\"\"\"\n",
        "        if self.current_step >= len(self.data) - 1:\n",
        "            return self._get_observation(), 0, True, {}\n",
        "\n",
        "        current_price = self.data['Close'].iloc[self.current_step]\n",
        "\n",
        "        # Execute action\n",
        "        if action == 1:  # Buy\n",
        "            shares_to_buy = self.balance // (current_price * (1 + self.transaction_cost))\n",
        "            if shares_to_buy > 0:\n",
        "                cost = shares_to_buy * current_price * (1 + self.transaction_cost)\n",
        "                self.balance -= cost\n",
        "                self.shares_held += shares_to_buy\n",
        "                self.trades.append({\n",
        "                    'step': self.current_step,\n",
        "                    'action': 'BUY',\n",
        "                    'shares': shares_to_buy,\n",
        "                    'price': current_price,\n",
        "                    'cost': cost\n",
        "                })\n",
        "\n",
        "        elif action == 2:  # Sell\n",
        "            if self.shares_held > 0:\n",
        "                revenue = self.shares_held * current_price * (1 - self.transaction_cost)\n",
        "                self.balance += revenue\n",
        "                self.total_sales_value += revenue\n",
        "                self.total_shares_sold += self.shares_held\n",
        "                self.trades.append({\n",
        "                    'step': self.current_step,\n",
        "                    'action': 'SELL',\n",
        "                    'shares': self.shares_held,\n",
        "                    'price': current_price,\n",
        "                    'revenue': revenue\n",
        "                })\n",
        "                self.shares_held = 0\n",
        "\n",
        "        # Move to next step\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate reward\n",
        "        new_net_worth = self.balance + self.shares_held * self.data['Close'].iloc[self.current_step]\n",
        "        reward = (new_net_worth - self.net_worth) / self.initial_balance\n",
        "\n",
        "        # Update tracking variables\n",
        "        self.net_worth = new_net_worth\n",
        "        self.max_net_worth = max(self.max_net_worth, new_net_worth)\n",
        "\n",
        "        # Check if done\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        return self._get_observation(), reward, done, {\n",
        "            'net_worth': self.net_worth,\n",
        "            'balance': self.balance,\n",
        "            'shares_held': self.shares_held\n",
        "        }\n",
        "\n",
        "    def get_performance_metrics(self):\n",
        "        \"\"\"Calculate performance metrics\"\"\"\n",
        "        if len(self.trades) == 0:\n",
        "            return {\n",
        "                'total_return': 0,\n",
        "                'num_trades': 0,\n",
        "                'win_rate': 0,\n",
        "                'sharpe_ratio': 0,\n",
        "                'max_drawdown': 0\n",
        "            }\n",
        "\n",
        "        total_return = (self.net_worth - self.initial_balance) / self.initial_balance\n",
        "\n",
        "        # Calculate other metrics\n",
        "        returns = []\n",
        "        for i in range(1, len(self.data)):\n",
        "            if i <= self.current_step:\n",
        "                price_change = (self.data['Close'].iloc[i] - self.data['Close'].iloc[i-1]) / self.data['Close'].iloc[i-1]\n",
        "                returns.append(price_change)\n",
        "\n",
        "        returns = np.array(returns)\n",
        "        sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0\n",
        "\n",
        "        # Max drawdown\n",
        "        max_drawdown = (self.max_net_worth - self.net_worth) / self.max_net_worth\n",
        "\n",
        "        return {\n",
        "            'total_return': total_return,\n",
        "            'num_trades': len(self.trades),\n",
        "            'sharpe_ratio': sharpe_ratio,\n",
        "            'max_drawdown': max_drawdown,\n",
        "            'final_balance': self.balance,\n",
        "            'final_shares': self.shares_held,\n",
        "            'net_worth': self.net_worth\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DEEP REINFORCEMENT LEARNING AGENTS\n",
        "# ============================================================================\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Neural Network\n",
        "        self.q_network = self._build_network()\n",
        "        self.target_network = self._build_network()\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Update target network\n",
        "        self.update_target_network()\n",
        "\n",
        "    def _build_network(self):\n",
        "        \"\"\"Build DQN neural network\"\"\"\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.action_size)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Update target network with main network weights\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store experience in replay buffer\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = self.q_network(state_tensor)\n",
        "        return np.argmax(q_values.cpu().data.numpy())\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        \"\"\"Train the model on a batch of experiences\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states = torch.FloatTensor([e[0] for e in batch])\n",
        "        actions = torch.LongTensor([e[1] for e in batch])\n",
        "        rewards = torch.FloatTensor([e[2] for e in batch])\n",
        "        next_states = torch.FloatTensor([e[3] for e in batch])\n",
        "        dones = torch.BoolTensor([e[4] for e in batch])\n",
        "\n",
        "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
        "        target_q_values = rewards + (0.95 * next_q_values * ~dones)\n",
        "\n",
        "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.0003):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Actor-Critic networks\n",
        "        self.actor = self._build_actor()\n",
        "        self.critic = self._build_critic()\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)\n",
        "\n",
        "        # PPO hyperparameters\n",
        "        self.clip_epsilon = 0.2\n",
        "        self.ppo_epochs = 4\n",
        "        self.entropy_coef = 0.01\n",
        "\n",
        "    def _build_actor(self):\n",
        "        \"\"\"Build actor network\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, self.action_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def _build_critic(self):\n",
        "        \"\"\"Build critic network\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Choose action using policy\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action_probs = self.actor(state_tensor)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action).item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        \"\"\"Evaluate state-action pair\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state)\n",
        "        action_tensor = torch.LongTensor(action)\n",
        "\n",
        "        action_probs = self.actor(state_tensor)\n",
        "        dist = Categorical(action_probs)\n",
        "        action_log_probs = dist.log_prob(action_tensor)\n",
        "        entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.critic(state_tensor)\n",
        "\n",
        "        return action_log_probs, state_value, entropy\n",
        "\n",
        "    def update(self, states, actions, rewards, log_probs, values, next_values):\n",
        "        \"\"\"Update policy using PPO\"\"\"\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        old_log_probs = torch.FloatTensor(log_probs)\n",
        "        values = torch.FloatTensor(values)\n",
        "        next_values = torch.FloatTensor(next_values)\n",
        "\n",
        "        # Calculate advantages\n",
        "        advantages = rewards + 0.99 * next_values - values\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # PPO update\n",
        "        for _ in range(self.ppo_epochs):\n",
        "            log_probs_new, values_new, entropy = self.evaluate(states, actions)\n",
        "\n",
        "            # Actor loss\n",
        "            ratio = torch.exp(log_probs_new - old_log_probs)\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy.mean()\n",
        "\n",
        "            # Critic loss\n",
        "            critic_loss = F.mse_loss(values_new.squeeze(), rewards + 0.99 * next_values)\n",
        "\n",
        "            # Update networks\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward(retain_graph=True)\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "# ============================================================================\n",
        "# 4. TRAINING FRAMEWORK\n",
        "# ============================================================================\n",
        "\n",
        "class TradingTrainer:\n",
        "    def __init__(self, data_pipeline, agent_type='DQN'):\n",
        "        self.data_pipeline = data_pipeline\n",
        "        self.agent_type = agent_type\n",
        "        self.results = {}\n",
        "\n",
        "    def train_agent(self, symbol, episodes=1000):\n",
        "        \"\"\"Train agent on specific symbol\"\"\"\n",
        "        print(f\"\\nTraining {self.agent_type} agent on {symbol}...\")\n",
        "\n",
        "        # Prepare data\n",
        "        data = self.data_pipeline.processed_data[symbol]['normalized']\n",
        "\n",
        "        # Split data\n",
        "        train_size = int(len(data) * 0.8)\n",
        "        train_data = data[:train_size]\n",
        "        test_data = data[train_size:]\n",
        "\n",
        "        # Create environment\n",
        "        env = TradingEnvironment(train_data)\n",
        "\n",
        "        # Initialize agent\n",
        "        if self.agent_type == 'DQN':\n",
        "            agent = DQNAgent(env.observation_space, env.action_space)\n",
        "        elif self.agent_type == 'PPO':\n",
        "            agent = PPOAgent(env.observation_space, env.action_space)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown agent type: {self.agent_type}\")\n",
        "\n",
        "        # Training loop\n",
        "        episode_rewards = []\n",
        "        episode_returns = []\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "\n",
        "            # For PPO\n",
        "            if self.agent_type == 'PPO':\n",
        "                states, actions, rewards, log_probs, values = [], [], [], [], []\n",
        "\n",
        "            while not done:\n",
        "                if self.agent_type == 'DQN':\n",
        "                    action = agent.act(state)\n",
        "                    next_state, reward, done, _ = env.step(action)\n",
        "                    agent.remember(state, action, reward, next_state, done)\n",
        "                    state = next_state\n",
        "                    episode_reward += reward\n",
        "\n",
        "                    if len(agent.memory) > 32:\n",
        "                        agent.replay()\n",
        "\n",
        "                elif self.agent_type == 'PPO':\n",
        "                    action, log_prob = agent.act(state)\n",
        "                    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                    # Store transition\n",
        "                    states.append(state)\n",
        "                    actions.append(action)\n",
        "                    rewards.append(reward)\n",
        "                    log_probs.append(log_prob)\n",
        "                    values.append(agent.critic(torch.FloatTensor(state).unsqueeze(0)).item())\n",
        "\n",
        "                    state = next_state\n",
        "                    episode_reward += reward\n",
        "\n",
        "            # Update PPO agent\n",
        "            if self.agent_type == 'PPO' and len(states) > 0:\n",
        "                next_values = values[1:] + [0]  # Bootstrap with 0 for terminal state\n",
        "                agent.update(states, actions, rewards, log_probs, values, next_values)\n",
        "\n",
        "            # Update target network for DQN\n",
        "            if self.agent_type == 'DQN' and episode % 100 == 0:\n",
        "                agent.update_target_network()\n",
        "\n",
        "            # Track progress\n",
        "            episode_rewards.append(episode_reward)\n",
        "            performance = env.get_performance_metrics()\n",
        "            episode_returns.append(performance['total_return'])\n",
        "\n",
        "            if episode % 100 == 0:\n",
        "                avg_reward = np.mean(episode_rewards[-100:])\n",
        "                avg_return = np.mean(episode_returns[-100:])\n",
        "                print(f\"Episode {episode}, Avg Reward: {avg_reward:.4f}, Avg Return: {avg_return:.4f}\")\n",
        "\n",
        "        # Test on unseen data\n",
        "        print(f\"Testing {self.agent_type} agent on {symbol}...\")\n",
        "        test_env = TradingEnvironment(test_data)\n",
        "        test_state = test_env.reset()\n",
        "        test_done = False\n",
        "\n",
        "        while not test_done:\n",
        "            if self.agent_type == 'DQN':\n",
        "                # Use trained policy without exploration\n",
        "                agent.epsilon = 0\n",
        "                test_action = agent.act(test_state)\n",
        "            else:  # PPO\n",
        "                test_action, _ = agent.act(test_state)\n",
        "\n",
        "            test_state, _, test_done, _ = test_env.step(test_action)\n",
        "\n",
        "        # Store results\n",
        "        test_performance = test_env.get_performance_metrics()\n",
        "        self.results[symbol] = {\n",
        "            'agent_type': self.agent_type,\n",
        "            'training_rewards': episode_rewards,\n",
        "            'training_returns': episode_returns,\n",
        "            'test_performance': test_performance,\n",
        "            'test_env': test_env,\n",
        "            'agent': agent\n",
        "        }\n",
        "\n",
        "        print(f\"✓ {symbol} - Test Return: {test_performance['total_return']:.4f}\")\n",
        "        return agent, test_performance\n",
        "\n",
        "# ============================================================================\n",
        "# 5. MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # Initialize data pipeline\n",
        "    pipeline = DataPipeline()\n",
        "\n",
        "    # Fetch and process data\n",
        "    pipeline.fetch_data()\n",
        "    pipeline.process_all_assets()\n",
        "\n",
        "    # Train agents for each asset\n",
        "    results = {}\n",
        "\n",
        "    for agent_type in ['DQN', 'PPO']:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training {agent_type} agents\")\n",
        "        print('='*60)\n",
        "\n",
        "        trainer = TradingTrainer(pipeline, agent_type)\n",
        "\n",
        "        for symbol in pipeline.assets:\n",
        "            if symbol in pipeline.processed_data:\n",
        "                agent, performance = trainer.train_agent(symbol, episodes=500)\n",
        "                results[f\"{agent_type}_{symbol}\"] = {\n",
        "                    'agent': agent,\n",
        "                    'performance': performance,\n",
        "                    'trainer': trainer\n",
        "                }\n",
        "\n",
        "    # Display results summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print('='*60)\n",
        "\n",
        "    for key, result in results.items():\n",
        "        agent_type, symbol = key.split('_', 1)\n",
        "        performance = result['performance']\n",
        "        print(f\"{agent_type} - {symbol}:\")\n",
        "        print(f\"  Total Return: {performance['total_return']:.4f}\")\n",
        "        print(f\"  Sharpe Ratio: {performance['sharpe_ratio']:.4f}\")\n",
        "        print(f\"  Number of Trades: {performance['num_trades']}\")\n",
        "        print(f\"  Max Drawdown: {performance['max_drawdown']:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return pipeline, results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline, results = main()\n",
        "    print(\"Training completed! Results stored in 'results' variable.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFbiinejGCIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Import our main trading system\n",
        "from drl_trading_system import DataPipeline, TradingEnvironment, DQNAgent, PPOAgent, TradingTrainer\n",
        "\n",
        "# Configure Streamlit page\n",
        "st.set_page_config(\n",
        "    page_title=\"DRL Trading System Dashboard\",\n",
        "    page_icon=\"📈\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Custom CSS\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        font-size: 2.5rem;\n",
        "        font-weight: bold;\n",
        "        color: #1f77b4;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    .metric-container {\n",
        "        background-color: #f0f2f6;\n",
        "        padding: 1rem;\n",
        "        border-radius: 0.5rem;\n",
        "        margin: 0.5rem 0;\n",
        "    }\n",
        "    .performance-card {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white;\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 1rem;\n",
        "        margin: 1rem 0;\n",
        "        text-align: center;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state\n",
        "if 'pipeline' not in st.session_state:\n",
        "    st.session_state.pipeline = None\n",
        "if 'results' not in st.session_state:\n",
        "    st.session_state.results = {}\n",
        "if 'trained_agents' not in st.session_state:\n",
        "    st.session_state.trained_agents = {}\n",
        "\n",
        "# Main header\n",
        "st.markdown('<h1 class=\"main-header\">🚀 Deep Reinforcement Learning Trading System</h1>', unsafe_allow_html=True)\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"Navigation\")\n",
        "page = st.sidebar.selectbox(\"Choose a page\", [\n",
        "    \"Overview\",\n",
        "    \"Data Pipeline\",\n",
        "    \"Agent Training\",\n",
        "    \"Performance Analysis\",\n",
        "    \"Live Trading Simulation\",\n",
        "    \"Model Comparison\"\n",
        "])\n",
        "\n",
        "# Data loading functions\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    \"\"\"Load and process data with caching\"\"\"\n",
        "    pipeline = DataPipeline()\n",
        "    pipeline.fetch_data()\n",
        "    pipeline.process_all_assets()\n",
        "    return pipeline\n",
        "\n",
        "def save_results(results, filename=\"trading_results.pkl\"):\n",
        "    \"\"\"Save results to file\"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "def load_results(filename=\"trading_results.pkl\"):\n",
        "    \"\"\"Load results from file\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return {}\n",
        "\n",
        "# ============================================================================\n",
        "# OVERVIEW PAGE\n",
        "# ============================================================================\n",
        "\n",
        "if page == \"Overview\":\n",
        "    st.header(\"📊 System Overview\")\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"performance-card\">\n",
        "            <h3>🎯 Objective</h3>\n",
        "            <p>Develop and compare DRL agents for automated stock trading using technical indicators and market data.</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"performance-card\">\n",
        "            <h3>🤖 Agents</h3>\n",
        "            <p>DQN (Deep Q-Network) and PPO (Proximal Policy Optimization) agents with neural network policies.</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    with col3:\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"performance-card\">\n",
        "            <h3>📈 Assets</h3>\n",
        "            <p>AAPL, MSFT, GOOGL, TSLA with 6 years of historical data and 25+ technical indicators.</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    st.subheader(\"🔧 System Architecture\")\n",
        "\n",
        "    # Architecture diagram (simplified)\n",
        "    fig = go.Figure()\n",
        "    fig.add_shape(\n",
        "        type=\"rect\", x0=0, y0=0, x1=2, y1=1,\n",
        "        line=dict(color=\"blue\"), fillcolor=\"lightblue\"\n",
        "    )\n",
        "    fig.add_annotation(x=1, y=0.5, text=\"Data Pipeline<br>Technical Indicators\", showarrow=False)\n",
        "\n",
        "    fig.add_shape(\n",
        "        type=\"rect\", x0=3, y0=0, x1=5, y1=1,\n",
        "        line=dict(color=\"green\"), fillcolor=\"lightgreen\"\n",
        "    )\n",
        "    fig.add_annotation(x=4, y=0.5, text=\"Trading Environment<br>State/Action/Reward\", showarrow=False)\n",
        "\n",
        "    fig.add_shape(\n",
        "        type=\"rect\", x0=6, y0=0, x1=8, y1=1,\n",
        "        line=dict(color=\"red\"), fillcolor=\"lightcoral\"\n",
        "    )\n",
        "    fig.add_annotation(x=7, y=0.5, text=\"DRL Agents<br>DQN/PPO\", showarrow=False)\n",
        "\n",
        "    # Add arrows\n",
        "    fig.add_annotation(x=2.5, y=0.5, text=\"→\", showarrow=False, font=dict(size=20))\n",
        "    fig.add_annotation(x=5.5, y=0.5, text=\"→\", showarrow=False, font=dict(size=20))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"DRL Trading System Architecture\",\n",
        "        xaxis=dict(range=[-0.5, 8.5], showgrid=False, showticklabels=False),\n",
        "        yaxis=dict(range=[-0.5, 1.5], showgrid=False, showticklabels=False),\n",
        "        height=200\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    st.subheader(\"📋 Key Features\")\n",
        "\n",
        "    features = [\n",
        "        \"🔍 Comprehensive technical indicator calculation (RSI, MACD, Bollinger Bands, etc.)\",\n",
        "        \"🧠 Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) agents\",\n",
        "        \"🎯 Multi-asset trading with risk management\",\n",
        "        \"📊 Real-time performance monitoring and visualization\",\n",
        "        \"🔄 Backtesting and forward testing capabilities\",\n",
        "        \"💾 Model persistence and result caching\"\n",
        "    ]\n",
        "\n",
        "    for feature in features:\n",
        "        st.write(feature)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PIPELINE PAGE\n",
        "# ============================================================================\n",
        "\n",
        "elif page == \"Data Pipeline\":\n",
        "    st.header(\"📈 Data Pipeline & Feature Engineering\")\n",
        "\n",
        "    # Load data button\n",
        "    if st.button(\"🔄 Load Fresh Data\"):\n",
        "        with st.spinner(\"Loading data...\"):\n",
        "            st.session_state.pipeline = load_data()\n",
        "        st.success(\"Data loaded successfully!\")\n",
        "\n",
        "    # Check if data is loaded\n",
        "    if st.session_state.pipeline is None:\n",
        "        st.warning(\"Please load data first using the button above.\")\n",
        "        st.stop()\n",
        "\n",
        "    pipeline = st.session_state.pipeline\n",
        "\n",
        "    # Data overview\n",
        "    st.subheader(\"📊 Data Overview\")\n",
        "\n",
        "    if pipeline.processed_data:\n",
        "        # Create summary table\n",
        "        summary_data = []\n",
        "        for symbol in pipeline.assets:\n",
        "            if symbol in pipeline.processed_data:\n",
        "                data = pipeline.processed_data[symbol]['clean']\n",
        "                summary_data.append({\n",
        "                    'Symbol': symbol,\n",
        "                    'Records': len(data),\n",
        "                    'Features': len(data.columns),\n",
        "                    'Start Date': data.index[0].strftime('%Y-%m-%d'),\n",
        "                    'End Date': data.index[-1].strftime('%Y-%m-%d'),\n",
        "                    'Latest Price': f\"${data['Close'].iloc[-1]:.2f}\"\n",
        "                })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        st.dataframe(summary_df, use_container_width=True)\n",
        "\n",
        "        # Feature categories\n",
        "        st.subheader(\"🔧 Feature Categories\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(\"\"\"\n",
        "            **Trend Indicators:**\n",
        "            - SMA (20, 50)\n",
        "            - EMA (20)\n",
        "            - MACD & Signal\n",
        "\n",
        "            **Momentum Indicators:**\n",
        "            - RSI (14)\n",
        "            - Stochastic (K, D)\n",
        "            - Williams %R\n",
        "            - ROC\n",
        "            \"\"\")\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(\"\"\"\n",
        "            **Volatility Indicators:**\n",
        "            - ATR (14)\n",
        "            - Bollinger Bands\n",
        "            - Volatility Regime\n",
        "\n",
        "            **Volume Indicators:**\n",
        "            - Volume SMA\n",
        "            - Volume Ratio\n",
        "            - Relative Volume\n",
        "            \"\"\")\n",
        "\n",
        "        # Interactive chart\n",
        "        st.subheader(\"📊 Interactive Price Chart\")\n",
        "\n",
        "        selected_symbol = st.selectbox(\"Select Symbol\", pipeline.assets)\n",
        "\n",
        "        if selected_symbol in pipeline.processed_data:\n",
        "            data = pipeline.processed_data[selected_symbol]['clean']\n",
        "\n",
        "            # Create candlestick chart\n",
        "            fig = make_subplots(\n",
        "                rows=3, cols=1,\n",
        "                subplot_titles=('Price & Moving Averages', 'RSI', 'MACD'),\n",
        "                vertical_spacing=0.1,\n",
        "                row_heights=[0.6, 0.2, 0.2]\n",
        "            )\n",
        "\n",
        "            # Candlestick chart\n",
        "            fig.add_trace(\n",
        "                go.Candlestick(\n",
        "                    x=data.index,\n",
        "                    open=data['Open'],\n",
        "                    high=data['High'],\n",
        "                    low=data['Low'],\n",
        "                    close=data['Close'],\n",
        "                    name='Price'\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "            # Moving averages\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=data.index, y=data['SMA_20'], name='SMA 20', line=dict(color='orange')),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=data.index, y=data['EMA_20'], name='EMA 20', line=dict(color='red')),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "            # RSI\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=data.index, y=data['RSI'], name='RSI', line=dict(color='purple')),\n",
        "                row=2, col=1\n",
        "            )\n",
        "            fig.add_hline(y=70, line_dash=\"dash\", line_color=\"red\", row=2, col=1)\n",
        "            fig.add_hline(y=30, line_dash=\"dash\", line_color=\"green\", row=2, col=1)\n",
        "\n",
        "            # MACD\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=data.index, y=data['MACD'], name='MACD', line=dict(color='blue')),\n",
        "                row=3, col=1\n",
        "            )\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=data.index, y=data['MACD_signal'], name='Signal', line=dict(color='red')),\n",
        "                row=3, col=1\n",
        "            )\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=f\"{selected_symbol} Technical Analysis\",\n",
        "                height=800,\n",
        "                showlegend=True\n",
        "            )\n",
        "\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "            # Correlation matrix\n",
        "            st.subheader(\"🔗 Feature Correlation Matrix\")\n",
        "\n",
        "            # Select key features for correlation\n",
        "            corr_features = ['Close', 'RSI', 'MACD', 'ATR', 'STOCH_K', 'Volume_ratio', 'BB_position']\n",
        "            corr_data = data[corr_features].corr()\n",
        "\n",
        "            fig_corr = px.imshow(\n",
        "                corr_data,\n",
        "                labels=dict(x=\"Features\", y=\"Features\", color=\"Correlation\"),\n",
        "                x=corr_features,\n",
        "                y=corr_features,\n",
        "                color_continuous_scale='RdBu',\n",
        "                aspect=\"auto\"\n",
        "            )\n",
        "            fig_corr.update_layout(title=\"Feature Correlation Matrix\")\n",
        "            st.plotly_chart(fig_corr, use_container_width=True)\n",
        "\n",
        "# ============================================================================\n",
        "# AGENT TRAINING PAGE\n",
        "# ============================================================================\n",
        "\n",
        "elif page == \"Agent Training\":\n",
        "    st.header(\"🤖 Agent Training & Configuration\")\n",
        "\n",
        "    # Check if data is loaded\n",
        "    if st.session_state.pipeline is None:\n",
        "        st.warning(\"Please load data first from the Data Pipeline page.\")\n",
        "        st.stop()\n",
        "\n",
        "    pipeline = st.session_state.pipeline\n",
        "\n",
        "    # Training configuration\n",
        "    st.subheader(\"⚙️ Training Configuration\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        agent_type = st.selectbox(\"Select Agent Type\", [\"DQN\", \"PPO\"])\n",
        "        selected_symbols = st.multiselect(\"Select Assets\", pipeline.assets, default=pipeline.assets[:2])\n",
        "        episodes = st.slider(\"Training Episodes\", 100, 2000, 500)\n",
        "\n",
        "    with col2:\n",
        "        learning_rate = st.number_input(\"Learning Rate\", 0.0001, 0.01, 0.001, format=\"%.4f\")\n",
        "        batch_size = st.number_input(\"Batch Size\", 16, 128, 32)\n",
        "\n",
        "        if agent_type == \"DQN\":\n",
        "            epsilon_decay = st.number_input(\"Epsilon Decay\", 0.990, 0.999, 0.995, format=\"%.3f\")\n",
        "        else:  # PPO\n",
        "            clip_epsilon = st.number_input(\"Clip Epsilon\", 0.1, 0.3, 0.2, format=\"%.1f\")\n",
        "\n",
        "    # Training button\n",
        "    if st.button(\"🚀 Start Training\"):\n",
        "        if not selected_symbols:\n",
        "            st.error(\"Please select at least one asset.\")\n",
        "            st.stop()\n",
        "\n",
        "        progress_bar = st.progress(0)\n",
        "        status_text = st.empty()\n",
        "\n",
        "        training_results = {}\n",
        "\n",
        "        for i, symbol in enumerate(selected_symbols):\n",
        "            status_text.text(f\"Training {agent_type} agent on {symbol}...\")\n",
        "\n",
        "            # Create trainer\n",
        "            trainer = TradingTrainer(pipeline, agent_type)\n",
        "\n",
        "            # Train agent\n",
        "            with st.spinner(f\"Training {agent_type} on {symbol}...\"):\n",
        "                agent, performance = trainer.train_agent(symbol, episodes)\n",
        "\n",
        "                training_results[f\"{agent_type}_{symbol}\"] = {\n",
        "                    'agent': agent,\n",
        "                    'performance': performance,\n",
        "                    'trainer': trainer\n",
        "                }\n",
        "\n",
        "            progress_bar.progress((i + 1) / len(selected_symbols))\n",
        "\n",
        "        # Store results\n",
        "        st.session_state.results.update(training_results)\n",
        "        st.session_state.trained_agents.update(training_results)\n",
        "\n",
        "        # Save results\n",
        "        save_results(st.session_state.results)\n",
        "\n",
        "        status_text.text(\"Training completed!\")\n",
        "        st.success(f\"Successfully trained {agent_type} agents on {len(selected_symbols)} assets!\")\n",
        "\n",
        "        # Display training summary\n",
        "        st.subheader(\"📊 Training Summary\")\n",
        "\n",
        "        summary_data = []\n",
        "        for key, result in training_results.items():\n",
        "            agent_type_name, symbol = key.split('_', 1)\n",
        "            performance = result['performance']\n",
        "            summary_data.append({\n",
        "                'Agent': agent_type_name,\n",
        "                'Symbol': symbol,\n",
        "                'Total Return': f\"{performance['total_return']:.2%}\",\n",
        "                'Sharpe Ratio': f\"{performance['sharpe_ratio']:.3f}\",\n",
        "                'Trades': performance['num_trades'],\n",
        "                'Max Drawdown': f\"{performance['max_drawdown']:.2%}\"\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        st.dataframe(summary_df, use_container_width=True)\n",
        "\n",
        "    # Display existing results\n",
        "    if st.session_state.results:\n",
        "        st.subheader(\"🗂️ Existing Training Results\")\n",
        "\n",
        "        existing_results = []\n",
        "        for key, result in st.session_state.results.items():\n",
        "            try:\n",
        "                agent_type_name, symbol = key.split('_', 1)\n",
        "                performance = result['performance']\n",
        "                existing_results.append({\n",
        "                    'Agent': agent_type_name,\n",
        "                    'Symbol': symbol,\n",
        "                    'Total Return': f\"{performance['total_return']:.2%}\",\n",
        "                    'Sharpe Ratio': f\"{performance['sharpe_ratio']:.3f}\",\n",
        "                    'Trades': performance['num_trades'],\n",
        "                    'Max Drawdown': f\"{performance['max_drawdown']:.2%}\"\n",
        "                })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if existing_results:\n",
        "            existing_df = pd.DataFrame(existing_results)\n",
        "            st.dataframe(existing_df, use_container_width=True)\n",
        "\n",
        "            # Clear results button\n",
        "            if st.button(\"🗑️ Clear All Results\"):\n",
        "                st.session_state.results = {}\n",
        "                st.session_state.trained_agents = {}\n",
        "                st.success(\"All results cleared!\")\n",
        "\n",
        "# ============================================================================\n",
        "# PERFORMANCE ANALYSIS PAGE\n",
        "# ============================================================================\n",
        "\n",
        "elif page == \"Performance Analysis\":\n",
        "    st.header(\"📊 Performance Analysis\")\n",
        "\n",
        "    # Check if results exist\n",
        "    if not st.session_state.results:\n",
        "        st.warning(\"No trained agents found. Please train some agents first.\")\n",
        "        st.stop()\n",
        "\n",
        "    # Results selector\n",
        "    result_keys = list(st.session_state.results.keys())\n",
        "    selected_result = st.selectbox(\"Select Trained Agent\", result_keys)\n",
        "\n",
        "    if selected_result:\n",
        "        result = st.session_state.results[selected_result]\n",
        "        performance = result['performance']\n",
        "        trainer = result['trainer']\n",
        "\n",
        "        # Performance metrics\n",
        "        st.subheader(\"📈 Performance Metrics\")\n",
        "\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "        with col1:\n",
        "            st.metric(\"Total Return\", f\"{performance['total_return']:.2%}\")\n",
        "\n",
        "        with col2:\n",
        "            st.metric(\"Sharpe Ratio\", f\"{performance['sharpe_ratio']:.3f}\")\n",
        "\n",
        "        with col3:\n",
        "            st.metric(\"Number of Trades\", performance['num_trades'])\n",
        "\n",
        "        with col4:\n",
        "            st.metric(\"Max Drawdown\", f\"{performance['max_drawdown']:.2%}\")\n",
        "\n",
        "        # Additional metrics\n",
        "        col5, col6, col7, col8 = st.columns(4)\n",
        "\n",
        "        with col5:\n",
        "            st.metric(\"Final Balance\", f\"${performance['final_balance']:.2f}\")\n",
        "\n",
        "        with col6:\n",
        "            st.metric(\"Final Shares\", performance['final_shares'])\n",
        "\n",
        "        with col7:\n",
        "            st.metric(\"Net Worth\", f\"${performance['net_worth']:.2f}\")\n",
        "\n",
        "        with col8:\n",
        "            roi = (performance['net_worth'] - 10000) / 10000\n",
        "            st.metric(\"ROI\", f\"{roi:.2%}\")\n",
        "\n",
        "        # Training progress charts\n",
        "        if selected_result in trainer.results:\n",
        "            training_data = trainer.results[selected_result.split('_', 1)[1]]\n",
        "\n",
        "            st.subheader(\"📊 Training Progress\")\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            with col1:\n",
        "                # Training rewards\n",
        "                fig_rewards = go.Figure()\n",
        "                fig_rewards.add_trace(go.Scatter(\n",
        "                    y=training_data['training_rewards'],\n",
        "                    mode='lines',\n",
        "                    name='Episode Rewards',\n",
        "                    line=dict(color='blue')\n",
        "                ))\n",
        "\n",
        "                # Add moving average\n",
        "                window = 50\n",
        "                if len(training_data['training_rewards']) > window:\n",
        "                    ma_rewards = np.convolve(training_data['training_rewards'],\n",
        "                                           np.ones(window)/window, mode='valid')\n",
        "                    fig_rewards.add_trace(go.Scatter(\n",
        "                        y=ma_rewards,\n",
        "                        mode='lines',\n",
        "                        name=f'MA({window})',\n",
        "                        line=dict(color='red')\n",
        "                    ))\n",
        "\n",
        "                fig_rewards.update_layout(\n",
        "                    title=\"Training Rewards\",\n",
        "                    xaxis_title=\"Episode\",\n",
        "                    yaxis_title=\"Reward\"\n",
        "                )\n",
        "                st.plotly_chart(fig_rewards, use_container_width=True)\n",
        "\n",
        "            with col2:\n",
        "                # Training returns\n",
        "                fig_returns = go.Figure()\n",
        "                fig_returns.add_trace(go.Scatter(\n",
        "                    y=training_data['training_returns'],\n",
        "                    mode='lines',\n",
        "                    name='Episode Returns',\n",
        "                    line=dict(color='green')\n",
        "                ))\n",
        "\n",
        "                # Add moving average\n",
        "                if len(training_data['training_returns']) > window:\n",
        "                    ma_returns = np.convolve(training_data['training_returns'],\n",
        "                                           np.ones(window)/window, mode='valid')\n",
        "                    fig_returns.add_trace(go.Scatter(\n",
        "                        y=ma_returns,\n",
        "                        mode='lines',\n",
        "                        name=f'MA({window})',\n",
        "                        line=dict(color='orange')\n",
        "                    ))\n",
        "\n",
        "                fig_returns.update_layout(\n",
        "                    title=\"Training Returns\",\n",
        "                    xaxis_title=\"Episode\",\n",
        "                    yaxis_title=\"Return\"\n",
        "                )\n",
        "                st.plotly_chart(fig_returns, use_container_width=True)\n",
        "\n",
        "        # Trading actions visualization\n",
        "        st.subheader(\"🎯 Trading Actions\")\n",
        "\n",
        "        if 'test_env' in training_data:\n",
        "            test_env = training_data['test_env']\n",
        "\n",
        "            if test_env.trades:\n",
        "                # Create trades DataFrame\n",
        "                trades_df = pd.DataFrame(test_env.trades)\n",
        "\n",
        "                # Trading actions pie chart\n",
        "                action_counts = trades_df['action'].value_counts()\n",
        "\n",
        "                fig_actions = go.Figure(data=[go.Pie(\n",
        "                    labels=action_counts.index,\n",
        "                    values=action_counts.values,\n",
        "                    hole=0.3\n",
        "                )])\n",
        "\n",
        "                fig_actions.update_layout(\n",
        "                    title=\"Trading Actions Distribution\",\n",
        "                    showlegend=True\n",
        "                )\n",
        "                st.plotly_chart(fig_actions, use_container_width=True)\n",
        "\n",
        "                # Trades table\n",
        "                st.subheader(\"📋 Trade History\")\n",
        "                st.dataframe(trades_df, use_container_width=True)\n",
        "\n",
        "        # Comparison with buy-and-hold\n",
        "        st.subheader(\"🆚 Buy-and-Hold Comparison\")\n",
        "\n",
        "        agent_type, symbol = selected_result.split('_', 1)\n",
        "\n",
        "        # Get original data\n",
        "        if st.session_state.pipeline and symbol in st.session_state.pipeline.processed_data:\n",
        "            original_data = st.session_state.pipeline.processed_data[symbol]['clean']\n",
        "\n",
        "            # Calculate buy-and-hold return\n",
        "            train_size = int(len(original_data) * 0.8)\n",
        "            test_data = original_data[train_size:]\n",
        "\n",
        "            if len(test_data) > 0:\n",
        "                buy_hold_return = (test_data['Close'].iloc[-1] - test_data['Close'].iloc[0]) / test_data['Close'].iloc[0]\n",
        "\n",
        "                comparison_data = {\n",
        "                    'Strategy': ['DRL Agent', 'Buy & Hold'],\n",
        "                    'Return': [performance['total_return'], buy_hold_return],\n",
        "                    'Sharpe': [performance['sharpe_ratio'], 0]  # Simplified\n",
        "                }\n",
        "\n",
        "                comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "                fig_comparison = px.bar(\n",
        "                    comparison_df,\n",
        "                    x='Strategy',\n",
        "                    y='Return',\n",
        "                    title='DRL Agent vs Buy & Hold Performance',\n",
        "                    color='Strategy'\n",
        "                )\n",
        "\n",
        "                st.plotly_chart(fig_comparison, use_container_width=True)\n",
        "\n",
        "                # Performance summary\n",
        "                if performance['total_return'] > buy_hold_return:\n",
        "                    st.success(f\"🎉 DRL Agent outperformed Buy & Hold by {(performance['total_return'] - buy_hold_return):.2%}\")\n",
        "                else:\n",
        "                    st.info(f\"📊 Buy & Hold outperformed DRL Agent by {(buy_hold_return - performance['total_return']):.2%}\")\n",
        "\n",
        "# ============================================================================\n",
        "# LIVE TRADING SIMULATION PAGE\n",
        "# ============================================================================\n",
        "\n",
        "elif page == \"Live Trading Simulation\":\n",
        "    st.header(\"🔴 Live Trading Simulation\")\n",
        "\n",
        "    # Check if agents are trained\n",
        "    if not st.session_state.trained_agents:\n",
        "        st.warning(\"No trained agents found. Please train some agents first.\")\n",
        "        st.stop()\n",
        "\n",
        "    # Agent selector\n",
        "    agent_keys = list(st.session_state.trained_agents.keys())\n",
        "    selected_agent = st.selectbox(\"Select Trained Agent\", agent_keys)\n",
        "\n",
        "    if selected_agent:\n",
        "        st.subheader(\"🎮 Simulation Controls\")\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            initial_balance = st.number_input(\"Initial Balance ($)\", 1000, 100000, 10000)\n",
        "\n",
        "        with col2:\n",
        "            transaction_cost = st.number_input(\"Transaction Cost (%)\", 0.0, 1.0, 0.1, format=\"%.2f\") / 100\n",
        "\n",
        "        with col3:\n",
        "            simulation_days = st.slider(\"Simulation Days\", 30, 365, 90)\n",
        "\n",
        "        # Start simulation button\n",
        "        if st.button(\"🚀 Start Live Simulation\"):\n",
        "            agent_type, symbol = selected_agent.split('_', 1)\n",
        "\n",
        "            # Get latest data\n",
        "            try:\n",
        "                with st.spinner(\"Fetching latest market data...\"):\n",
        "                    end_date = datetime.now()\n",
        "                    start_date = end_date - timedelta(days=simulation_days + 100)  # Extra days for indicators\n",
        "\n",
        "                    ticker = yf.Ticker(symbol)\n",
        "                    latest_data = ticker.history(start=start_date, end=end_date)\n",
        "\n",
        "                    if len(latest_data) > 100:\n",
        "                        # Apply feature engineering\n",
        "                        pipeline = st.session_state.pipeline\n",
        "                        processed_latest = pipeline.calculate_technical_indicators(latest_data)\n",
        "\n",
        "                        # Take last simulation_days for actual simulation\n",
        "                        sim_data = processed_latest[-simulation_days:].copy()\n",
        "\n",
        "                        # Normalize using stored scalers\n",
        "                        if symbol in pipeline.scalers:\n",
        "                            price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "                            tech_cols = [col for col in sim_data.columns if col not in price_cols]\n",
        "\n",
        "                            # Note: In production, you'd want to update scalers with recent data\n",
        "                            # For demo, we'll use the stored scalers\n",
        "                            sim_data_norm = sim_data.copy()\n",
        "\n",
        "                            # Create simulation environment\n",
        "                            sim_env = TradingEnvironment(sim_data_norm, initial_balance, transaction_cost)\n",
        "\n",
        "                            # Run simulation\n",
        "                            agent_result = st.session_state.trained_agents[selected_agent]\n",
        "                            agent = agent_result['agent']\n",
        "\n",
        "                            # Simulate trading\n",
        "                            state = sim_env.reset()\n",
        "                            done = False\n",
        "\n",
        "                            portfolio_values = [initial_balance]\n",
        "                            actions_taken = []\n",
        "\n",
        "                            while not done:\n",
        "                                if agent_type == 'DQN':\n",
        "                                    agent.epsilon = 0  # No exploration in simulation\n",
        "                                    action = agent.act(state)\n",
        "                                else:  # PPO\n",
        "                                    action, _ = agent.act(state)\n",
        "\n",
        "                                actions_taken.append(action)\n",
        "                                state, reward, done, info = sim_env.step(action)\n",
        "                                portfolio_values.append(info['net_worth'])\n",
        "\n",
        "                            # Display results\n",
        "                            final_performance = sim_env.get_performance_metrics()\n",
        "\n",
        "                            st.subheader(\"📊 Simulation Results\")\n",
        "\n",
        "                            col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "                            with col1:\n",
        "                                st.metric(\"Final Return\", f\"{final_performance['total_return']:.2%}\")\n",
        "\n",
        "                            with col2:\n",
        "                                st.metric(\"Total Trades\", final_performance['num_trades'])\n",
        "\n",
        "                            with col3:\n",
        "                                st.metric(\"Final Balance\", f\"${final_performance['final_balance']:.2f}\")\n",
        "\n",
        "                            with col4:\n",
        "                                st.metric(\"Net Worth\", f\"${final_performance['net_worth']:.2f}\")\n",
        "\n",
        "                            # Portfolio value chart\n",
        "                            fig_portfolio = go.Figure()\n",
        "\n",
        "                            # Portfolio value\n",
        "                            fig_portfolio.add_trace(go.Scatter(\n",
        "                                x=list(range(len(portfolio_values))),\n",
        "                                y=portfolio_values,\n",
        "                                mode='lines',\n",
        "                                name='Portfolio Value',\n",
        "                                line=dict(color='blue', width=2)\n",
        "                            ))\n",
        "\n",
        "                            # Buy and hold comparison\n",
        "                            buy_hold_values = [initial_balance * (sim_data['Close'].iloc[i] / sim_data['Close'].iloc[0]) for i in range(len(sim_data))]\n",
        "                            buy_hold_values = [initial_balance] + buy_hold_values\n",
        "\n",
        "                            fig_portfolio.add_trace(go.Scatter(\n",
        "                                x=list(range(len(buy_hold_values))),\n",
        "                                y=buy_hold_values,\n",
        "                                mode='lines',\n",
        "                                name='Buy & Hold',\n",
        "                                line=dict(color='red', width=2, dash='dash')\n",
        "                            ))\n",
        "\n",
        "                            fig_portfolio.update_layout(\n",
        "                                title=f\"Portfolio Performance - {symbol}\",\n",
        "                                xaxis_title=\"Days\",\n",
        "                                yaxis_title=\"Portfolio Value ($)\",\n",
        "                                hovermode='x unified'\n",
        "                            )\n",
        "\n",
        "                            st.plotly_chart(fig_portfolio, use_container_width=True)\n",
        "\n",
        "                            # Action distribution\n",
        "                            action_names = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n",
        "                            action_counts = pd.Series(actions_taken).value_counts()\n",
        "                            action_labels = [action_names.get(i, f'Action {i}') for i in action_counts.index]\n",
        "\n",
        "                            fig_actions = go.Figure(data=[go.Pie(\n",
        "                                labels=action_labels,\n",
        "                                values=action_counts.values,\n",
        "                                hole=0.3\n",
        "                            )])\n",
        "\n",
        "                            fig_actions.update_layout(title=\"Trading Actions Distribution\")\n",
        "                            st.plotly_chart(fig_actions, use_container_width=True)\n",
        "\n",
        "                            # Trading log\n",
        "                            if sim_env.trades:\n",
        "                                st.subheader(\"📝 Trading Log\")\n",
        "                                trades_df = pd.DataFrame(sim_env.trades)\n",
        "                                trades_df['date'] = sim_data.index[trades_df['step']].strftime('%Y-%m-%d')\n",
        "                                st.dataframe(trades_df[['date', 'action', 'shares', 'price']], use_container_width=True)\n",
        "\n",
        "                    else:\n",
        "                        st.error(\"Insufficient recent data for simulation.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error in simulation: {str(e)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL COMPARISON PAGE\n",
        "# ============================================================================\n",
        "elif page == \"Model Comparison\":\n",
        "    st.header(\"🔍 Model Comparison\")\n",
        "\n",
        "    # Check if results exist\n",
        "    if not st.session_state.results:\n",
        "        st.warning(\"No trained agents found. Please train some agents first.\")\n",
        "        st.stop()\n",
        "\n",
        "    # Group results by agent type and symbol\n",
        "    dqn_results = {}\n",
        "    ppo_results = {}\n",
        "\n",
        "    for key, result in st.session_state.results.items():\n",
        "        try:\n",
        "            agent_type, symbol = key.split('_', 1)\n",
        "            if agent_type == 'DQN':\n",
        "                dqn_results[symbol] = result['performance']\n",
        "            elif agent_type == 'PPO':\n",
        "                ppo_results[symbol] = result['performance']\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Create comparison data\n",
        "    comparison_data = []\n",
        "\n",
        "    # Get common symbols\n",
        "    common_symbols = set(dqn_results.keys()) & set(ppo_results.keys())\n",
        "\n",
        "    if not common_symbols:\n",
        "        st.warning(\"No common symbols found for comparison. Please train both DQN and PPO agents on the same symbols.\")\n",
        "        st.stop()\n",
        "\n",
        "    for symbol in common_symbols:\n",
        "        dqn_perf = dqn_results[symbol]\n",
        "        ppo_perf = ppo_results[symbol]\n",
        "\n",
        "        comparison_data.extend([\n",
        "            {\n",
        "                'Agent': 'DQN',\n",
        "                'Symbol': symbol,\n",
        "                'Return': dqn_perf['total_return'],\n",
        "                'Sharpe': dqn_perf['sharpe_ratio'],\n",
        "                'Trades': dqn_perf['num_trades'],\n",
        "                'Max_Drawdown': dqn_perf['max_drawdown']\n",
        "            },\n",
        "            {\n",
        "                'Agent': 'PPO',\n",
        "                'Symbol': symbol,\n",
        "                'Return': ppo_perf['total_return'],\n",
        "                'Sharpe': ppo_perf['sharpe_ratio'],\n",
        "                'Trades': ppo_perf['num_trades'],\n",
        "                'Max_Drawdown': ppo_perf['max_drawdown']\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Plot comparison by return\n",
        "    st.subheader(\"📊 Return Comparison\")\n",
        "    fig_return = px.bar(\n",
        "        comparison_df,\n",
        "        x=\"Symbol\",\n",
        "        y=\"Return\",\n",
        "        color=\"Agent\",\n",
        "        barmode=\"group\",\n",
        "        text=comparison_df[\"Return\"].apply(lambda x: f\"{x:.2%}\"),\n",
        "        title=\"DQN vs PPO: Total Return by Symbol\"\n",
        "    )\n",
        "    fig_return.update_layout(yaxis_tickformat='.0%', xaxis_title=\"Asset\", yaxis_title=\"Total Return\")\n",
        "    st.plotly_chart(fig_return, use_container_width=True)\n",
        "\n",
        "    # Plot comparison by Sharpe Ratio\n",
        "    st.subheader(\"📈 Sharpe Ratio Comparison\")\n",
        "    fig_sharpe = px.bar(\n",
        "        comparison_df,\n",
        "        x=\"Symbol\",\n",
        "        y=\"Sharpe\",\n",
        "        color=\"Agent\",\n",
        "        barmode=\"group\",\n",
        "        text=comparison_df[\"Sharpe\"].round(2),\n",
        "        title=\"DQN vs PPO: Sharpe Ratio by Symbol\"\n",
        "    )\n",
        "    fig_sharpe.update_layout(xaxis_title=\"Asset\", yaxis_title=\"Sharpe Ratio\")\n",
        "    st.plotly_chart(fig_sharpe, use_container_width=True)\n",
        "\n",
        "    # Summary statistics\n",
        "    st.subheader(\"📊 Summary Statistics\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(\"**DQN Performance:**\")\n",
        "        dqn_avg_return = comparison_df[comparison_df['Agent'] == 'DQN']['Return'].mean()\n",
        "        dqn_avg_sharpe = comparison_df[comparison_df['Agent'] == 'DQN']['Sharpe'].mean()\n",
        "        st.write(f\"Average Return: {dqn_avg_return:.2%}\")\n",
        "        st.write(f\"Average Sharpe: {dqn_avg_sharpe:.3f}\")\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"**PPO Performance:**\")\n",
        "        ppo_avg_return = comparison_df[comparison_df['Agent'] == 'PPO']['Return'].mean()\n",
        "        ppo_avg_sharpe = comparison_df[comparison_df['Agent'] == 'PPO']['Sharpe'].mean()\n",
        "        st.write(f\"Average Return: {ppo_avg_return:.2%}\")\n",
        "        st.write(f\"Average Sharpe: {ppo_avg_sharpe:.3f}\")\n",
        "\n",
        "    # Tabular view\n",
        "    st.subheader(\"📋 Detailed Comparison Table\")\n",
        "    display_df = comparison_df.copy()\n",
        "    display_df[\"Return\"] = display_df[\"Return\"].apply(lambda x: f\"{x:.2%}\")\n",
        "    display_df[\"Sharpe\"] = display_df[\"Sharpe\"].round(3)\n",
        "    display_df[\"Max_Drawdown\"] = display_df[\"Max_Drawdown\"].apply(lambda x: f\"{x:.2%}\")\n",
        "\n",
        "    st.dataframe(display_df, use_container_width=True)"
      ],
      "metadata": {
        "id": "ueQDWXOYBBdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}